---
# tasks file for roles/apigee-hybrid-gke-create
- name: Set gke cache
  set_fact:
    cacheable: yes
    ATTACHED_CLUSTER: False
    NODE_COUNT: "{{ NODE_COUNT | default(DEFAULT_NODE_COUNT) }}"
    NODE_SIZE: "{{ NODE_SIZE | default(DEFAULT_NODE_SIZE) }}"
    GCLOUD_SERVICE_ACCOUNT_NAME: "{{ GCLOUD_SERVICE_ACCOUNT_NAME | default(DEFAULT_SERVICE_ACCOUNT_NAME) }}"

- name: Set gke cache
  set_fact:
    cacheable: yes
    SERVICE_ACCOUNT_KEY_PATH: "{{ WORK_DIR }}/{{ GCLOUD_SERVICE_ACCOUNT_NAME }}-service-account-credentials.json"

- name: Set KUBECONFIG cache
  set_fact:
    cacheable: yes
    FQ_CLUSTER_NAME: "gke_{{ PROJECT_ID }}_{{  CLUSTER_ZONE }}_{{ CLUSTER_NAME }}"
    CLUSTER_KUBECONFIG: "{{ WORK_DIR }}/{{ CLUSTER_NAME }}-{{ CLUSTER_ZONE }}.context.json"

- name: Confirm WORK_DIR
  file: 
    path: "{{ WORK_DIR }}"
    state: directory  

- name: Set default project
  shell: gcloud config set project "{{ PROJECT_ID }}"

- name: Set compute/zone
  shell: gcloud config set compute/zone {{ CLUSTER_ZONE }}

- name: Create gcp cluster APIGEE_VERSION < 1.10
  shell: |
    gcloud container clusters create {{ CLUSTER_NAME }} \
      --zone {{ CLUSTER_ZONE }} \
      --machine-type={{ NODE_SIZE }} \
      --num-nodes={{ NODE_COUNT | int }} \
      --min-nodes={{ (NODE_COUNT | int ) - 1 }} \
      --max-nodes={{ (NODE_COUNT | int ) * 2 }} \
      --subnetwork={{ VPC_NETWORK_NAME }} \
      --enable-ip-alias \
      --no-enable-autoupgrade \
      --no-enable-autorepair \
      --enable-autoscaling \
      --project {{ PROJECT_ID }}
  register: status 
  failed_when:  
    - status.rc == 1
    - "'Already exists' not in status.stderr"
  when: "{{ APIGEE_VERSION is version('1.10.0', '<') }}"

- name: Create gcp cluster APIGEE_VERSION >= 1.10
  shell: |
    gcloud beta container clusters create {{ CLUSTER_NAME }} \
      --project {{ PROJECT_ID }} \
      --region {{ REGION }} \
      --no-enable-basic-auth \
      --cluster-version {{ CLUSTER_VERSION }} \
      --release-channel {{ RELEASE_CHANNEL }} \
      --machine-type {{ NODE_SIZE }} \
      --image-type {{ IMAGE_TYPE }} \
      --disk-type {{ DISK_TYPE }} \
      --disk-size {{ DISK_SIZE }} \
      --metadata disable-legacy-endpoints=true \
      --service-account {{ GCLOUD_SERVICE_ACCOUNT_NAME }} \
      --num-nodes {{ NODE_COUNT | int }} \
      --max-nodes {{ (NODE_COUNT | int ) * 2 }} \
      --min-nodes {{ (NODE_COUNT | int ) - 1 }} \
      --logging={{ LOGGING }} \
      --monitoring={{ MONITORING }} \
      --enable-ip-alias \
      --network "{{ VPC_NETWORK_NAME }}" \
      --subnetwork "{{ VPC_SUBNETWORK_NAME }}" \
      --no-enable-intra-node-visibility \
      --default-max-pods-per-node "110" \
      --enable-autoscaling \
      --location-policy "BALANCED" \
      --security-posture=standard \
      --workload-vulnerability-scanning=disabled \
      --no-enable-master-authorized-networks \
      --addons {{ CLUSTER_ADDONS }} \
      --no-enable-autoupgrade \
      --no-enable-autorepair \
      --max-surge-upgrade {{ MAX_SURGE_UPGRADE }} \
      --max-unavailable-upgrade {{ MAX_UNAVAILABLE_UPGRADE }} \
      --binauthz-evaluation-mode=DISABLED \
      --enable-managed-prometheus \
      --enable-shielded-nodes \
      && \
    gcloud beta container node-pools create {{ NODE_POOL_NAME_DATA }} \
      --project "friasc-hybrid-20231025-1017" \
      --cluster {{ CLUSTER_NAME }} \
      --region {{ REGION }} \
      --machine-type {{ NODE_SIZE }} \
      --image-type {{ IMAGE_TYPE }} \
      --disk-type {{ DISK_TYPE }} \
      --disk-size {{ DISK_SIZE }} \
      --metadata disable-legacy-endpoints=true \
      --service-account {{ GCLOUD_SERVICE_ACCOUNT_NAME }} \
      --enable-autoscaling \
      --num-nodes {{ NODE_COUNT | int }} \
      --max-nodes {{ (NODE_COUNT | int ) * 2 }} \
      --min-nodes {{ (NODE_COUNT | int ) - 1 }} \
      --location-policy "BALANCED" \
      --no-enable-autoupgrade \
      --no-enable-autorepair \
      --max-surge-upgrade {{ MAX_SURGE_UPGRADE }} \
      --max-unavailable-upgrade {{ MAX_UNAVAILABLE_UPGRADE }} \
    && \
    gcloud beta container  node-pools create {{ NODE_POOL_NAME_RUNTIME \
      --project {{ PROJECT_ID }} \
      --cluster {{ CLUSTER_NAME }} \
      --region {{ REGION }} \
      --machine-type {{ NODE_SIZE }} \
      --image-type {{ IMAGE_TYPE }} \
      --disk-type {{ DISK_TYPE }} \
      --disk-size {{ DISK_SIZE }} \
      --metadata disable-legacy-endpoints=true \
      --service-account {{ GCP_SERVICE_ACCOUNT_NAME }} \
      --enable-autoscaling \
      --num-nodes {{ NODE_COUNT | int }} \
      --max-nodes {{ (NODE_COUNT | int ) * 2 }} \
      --min-nodes {{ (NODE_COUNT | int ) - 1 }} \
      --location-policy "BALANCED" \
      --no-enable-autoupgrade \
      --no-enable-autorepair \
      --max-surge-upgrade {{ MAX_SURGE_UPGRADE }} \
      --max-unavailable-upgrade {{ MAX_UNAVAILABLE_UPGRADE }}
  register: status
  failed_when:
  - status.rc == 1
  - "'Already exists' not in status.stderr"
  when: "{{ APIGEE_VERSION is version('1.10.0', '>=')}}"

- name: Obtain PROJECT_NUMBER
  shell: gcloud projects describe {{ PROJECT_ID }} --format="value(projectNumber)"
  register: project_number

- name: Create ASM service account
  ignore_errors: yes
  shell: gcloud iam service-accounts create {{ GCLOUD_SERVICE_ACCOUNT_NAME }} --project={{ PROJECT_ID }}
  register: status
  failed_when:
    - status.rc == 1
    - "'already exists' not in status.stderr"

- name: Bind gkehub.connect IAM role to service account - this is now in the install_asm bash script
  tags: ['install_asm_skip']
  shell: |
    gcloud projects add-iam-policy-binding {{ PROJECT_ID }} \
     --member="serviceAccount:{{ GCLOUD_SERVICE_ACCOUNT_EMAIL }}" \
     --role="roles/gkehub.connect"

- name: Download service account credential file
  shell: |
    gcloud iam service-accounts keys create {{ SERVICE_ACCOUNT_KEY_PATH }} \
      --iam-account="{{ GCLOUD_SERVICE_ACCOUNT_EMAIL }}"
  args:
    creates: "{{ SERVICE_ACCOUNT_KEY_PATH }}"

- name: Register the cluster - this is now in the install_asm bash script
  shell: |
    gcloud container hub memberships register {{ CLUSTER_NAME }} \
    --gke-cluster={{ CLUSTER_ZONE }}/{{ CLUSTER_NAME }} \
    --service-account-key-file={{ SERVICE_ACCOUNT_KEY_PATH }}
  register: status
  failed_when:
    - status.rc == 1
    - "'already exists' not in status.stderr"

- name: Set workload pool values
  set_fact:
    cacheable: yes
    WORKLOAD_POOL: "{{ PROJECT_ID }}.svc.id.goog"
    PROJECT_NUMBER: "{{ project_number.stdout }}"
    MESH_ID: "proj-{{ PROJECT_NUMBER | default(project_number.stdout) }}"

#- name: Set MESH_ID label - this is now in the install_asm bash script
#  tags: ['install_asm_skip']
#  shell: gcloud container clusters update {{ CLUSTER_NAME }} --update-labels=mesh_id={{ MESH_ID }}
#
#- name: Enable workload identity - this is now in the install_asm bash script
#  tags: ['install_asm_skip']
#  shell: gcloud container clusters update {{ CLUSTER_NAME }} --workload-pool={{ WORKLOAD_POOL }}
#
#- name: Enable Cloud Monitoring and Cloud Logging in GKE - this is now in the install_asm bash script
#  tags: ['install_asm_skip']
#  shell: gcloud container clusters update {{ CLUSTER_NAME }}

- block:
  - name: Rename cluster
    shell: |
      kubectx {{ CLUSTER_NAME }}={{ FQ_CLUSTER_NAME }}
      kubectx {{ CLUSTER_NAME }}
  rescue:
    - name: RESCUE - Wait for the cluster...
      pause:
        seconds: 15

    - name: RESCUE - Rename cluster
      shell: |
        kubectx {{ CLUSTER_NAME }}={{ FQ_CLUSTER_NAME }}
        kubectx {{ CLUSTER_NAME }}

- name: Generate kubeconfig file content
  shell: kubectl config view --minify --flatten --context={{ CLUSTER_NAME }}
  register: kubeconfig_content  

- name: Create kubeconfig file
  copy: 
    content: "{{ kubeconfig_content.stdout }}"
    dest: "{{ CLUSTER_KUBECONFIG }}"     

- name: Ensure you have cluster-admin on the cluster
  ignore_errors: yes
  shell: kubectl create clusterrolebinding user-cluster-admin --clusterrole cluster-admin --user {{ GCLOUD_ACCOUNT | default(GCLOUD_ACCOUNT_USER) }}
  failed_when:  
    - status.rc == 1
    - "'already exists' not in status.stderr"
    
- name: Update authentication credentials
  command: gcloud container clusters get-credentials {{ CLUSTER_NAME }} --region {{ REGION }}

- name: Prepare storageclass system path
  file:
    path: "{{ HYBRID_FILES_DIR }}"
    state: directory
    mode: 0755

- name: Copy storageclass file to system path
  copy:
    src: storageclass.yml
    dest: "{{ HYBRID_FILES_DIR }}/"

- name: Apply storageclass to cluster
  command: kubectl apply -f {{ HYBRID_FILES_DIR }}/storageclass.yml

- name: Apply storageclass patch 1 of 2
  command: "kubectl patch storageclass standard-rwo -p '{\"metadata\": {\"annotations\":{\"storageclass.kubernetes.io/is-default-class\":\"false\"}}}'"

